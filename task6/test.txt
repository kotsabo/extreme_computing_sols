Previously, we studied a compact acoustic model using highway
deep neural network (HDNN) for resource constrained speech recognition
[11]. HDNN is a type of network with shortcut connections
between hidden layers [12]. Compared to the plain networks with
skip connections, HDNNs are equipped with two gate functions –
transform and carry gate – to control and facilitate the information
flow over all the whole network. In particular, the transform gate is
used to scale the output of a hidden layer and the carry gate is used to
pass through the input directly after elementwise rescaling. The gate
functions are the key to train very deep networks [12] and to speed
up convergence as experimentally validated in [11]. We have shown
that the gate functions can manipulate the behavior of the whole neural
networks in sequence training and adaptation [13]. With the gate
functions, we can train much thinner and deeper networks with much
smaller number of model parameters, which can achieve comparable
recognition accuracy compared to much larger plain DNNs.
In this paper, we investigate teacher-student training to further
improve the accuracy of the small-footprint HDNN acoustic model.
In particular, we use a large size plain DNN acoustic model to provide
soft labels for training the student HDNN model. As mentioned
before, there have been a number of work on teacher-student training
for speech recognition [6, 7, 10]. The one that is closest to our study
is [6]. However, the student model investigated in this paper is much
smaller due to the highway architecture. In addition, we present further
analysis and experimental study on hybrid loss functions that
interpolate the cross-entropy and teacher-student costs, the use of
temperature to smooth the soft labels as well
where A(Y ; ^ Y ) measures the state level distance between the
ground truth and predicted labels;  denotes the hypothesis space
represented by a denominator lattice, and W is the word-level transcription;
k is the acoustic score scaling parameter. In this paper, we
only focus on the sMBR criterion since it can achieve comparable or
slightly better results compared to the maximum mutual information
(MMI) or minimum pone error (MPE) criterion [17].
For sequence training, the acoustic model is normally firstly
trained with the CE loss function, which is then fine tuned with the
sequence-level loss for a few iterations. While for knowledge distillation,
the model is firstly trained with the loss function as Eq. (8).
This may raise the question that if the improvement will diminish
in sequence training, and we will perform experimental study to answear
this question. Note that, only applying the sequence training
phone
(IHM) subset of the AMI meeting speech transcription corpus
[21].The amount of training data is around 80 hours, corresponding
to roughly 28 million frames. We followed the experimental
setup in [13]. We used 40-dimensional fMLLR adapted features vectors
normalised on the per-speaker level, which were then spliced
by a context window of 15 frames (i.e. 7). The number of tied
HMM states is 3927. The HDNN models were trained using the
CNTK toolkit [22], while the results were obtained using the Kaldi
decoder [23]. We also used the Kaldi tookit to compute the alignment
and lattices for sequence training. We set the momentum to
be 0.9 after the 1st epoch for CE training, and we used the sigmoid
activation for all the networks. The weights in each hidden layer of
HDNNs were randomly initialised with a uniform distribution in the
range of [􀀀0:5; 0:5] and the bias parameters were initialised to be 0
for CNTK systems. We used a trigram language model for decoding.
The word error rates (WERs) of the baseline systems with different
model structures are shown in Table 1.We then improved the teacher model by sMBR-based sequence
training, and used this model to supervise the training of the student
model. Similar to the observations in [6], the sMBR-based
flow over all the whole network. In particular, the transform gate is
used to scale the output of a hidden layer and the carry gate is used to
pass through the input directly after elementwise rescaling. The gate
functions are the key to train very deep networks [12] and to speed
up convergence as experimentally validated in [11]. We have shown
that the gate functions can manipulate the behavior of the whole neural
networks in sequence training and adaptation [13]. With the gate
functions, we can train much thinner and deeper networks with much
smaller number of model parameters, which can achieve comparable
recognition accuracy compared to much larger plain DNNs.
In this paper, we investigate teacher-student training to further
improve the accuracy of the small-footprint HDNN acoustic model.
In particular, we use a large size plain DNN acoustic model to provide
soft labels for training the student HDNN model. As mentioned
before, there have been a number of work on teacher-student training
for speech recognition [6, 7, 10]. The one that is closest to our study
is [6]. However, the student model investigated in this paper is much
smaller due to the highway architecture. In addition, we present further
analysis and experimental study on hybrid loss functions that
interpolate the cross-entropy and teacher-student costs, the use of
temperature to smooth the soft labels as well
where A(Y ; ^ Y ) measures the state level distance between the
ground truth and predicted labels;  denotes the hypothesis space
represented by a denominator lattice, and W is the word-level transcription;
k is the acoustic score scaling parameter. In this paper, we
only focus on the sMBR criterion since it can achieve comparable or
slightly better results compared to the maximum mutual information
(MMI) or minimum pone error (MPE) criterion [17].
For sequence training, the acoustic model is normally firstly
trained with the CE loss function, which is then fine tuned with the
sequence-level loss for a few iterations. While for knowledge distillation,
the model is firstly trained with the loss function as Eq. (8).
This may raise the question that if the improvement will diminish
in sequence training, and we will perform experimental study to answear
this question. Note that, only applying the sequence training
phone
teacher model can significantly improve the performance of the student
model. In fact, the error rate is lower than that achieved by the
student model trained independently with sMBR as shown in Table
3 (28.8% vs. 29.4% on the eval set). Note that, since the sequence
training criterion is not to maximise the frame accuracy, training the
model with this criterion normally reduces the frame accuracy, as
shown explicitly by Figure 6 in [24]. Interestingly, we observed the
same pattern in the case of teacher-student training. Figure 2 shows
the convergence curves of using CE and sMBR based teacher models,
where we see that the student model achieves much higher framefor the standalone HDNN model, i.e., we decoded the evaluation utterances
to obtain the hard labels first, and then used these labels to
adapt the model using the CE loss as Eq. (7). However, using the
teacher-student loss as Eq. (8), only one pass decoding is required
because the pseudo labels for adaptation are provided by the teacher
model, which does not need the word level transcription. This is a
particular advantage of the teacher-student training technique. However,
note that for resource constrained application scenarios, the student
model should be adapted offline, because otherwise the teacher
model needs to be accessed to generate the labels. This requires another
set of unlabelled speaker-dependent data for adaptation, but it
is usually not expensive to collect.
Since the standard AMI corpus does not have this additional set
of speaker-dependent data, we only show online adaptation results.
We used the teacher-student trained model from row 1 of Table 3
as the speaker-independent (SI) model because its pipeline is much
simpler. The baseline system used the same network as the SI model,
but it was trained independently. During adaptation, we updated the
SI model by 5 iterations with fixed learning rate as 2  10􀀀4 per
sample following our previous setup [13]. We also compared the CE
loss as Eq. (7) and the teacher-student loss as Eq. (8) for adaptation.
Results are given in Table 4. Using the CE loss function for both SI
models, only updating the gates yields slightly better results, while
updating all the model parameters gives smaller improvements, possibly
due to overfitting. Interestingly, this is not the case for the
teacher-student loss, i.e. updating all the model parameters yields
lower WER. These results may agree with the argument in [10] that
the soft targets can work as a regulariser
We then study if the accuracy of the student model can be further
improved by the sequence level criterion. Here, we set the smoothing
parameter p = 0:2 in Eq. (13) and the default learning rate to
be 1  10􀀀5 following our previous setup in [13]. Table 3 shows
the sequence training results of student models supervised by the
CE and sMBR-based teacher models respectively. Not surprisingly,
the student model supervised by the CE-based DNN model can be
significantly improved by the sequence training. Notably, the WER
obtained by this approach is lower compared to the model trained
independently with sMBR (28.4% vs. 29.4% on the eval set).
However, this configuration did not work for the student model supervised
by the sMBR-based teacher model. After inspection, we
found that it was due to overfitting. We then increased the value of
p for stronger regularisation and reduced the learning rate. Lower
WERs can be obtained as the table shows, however, the improvement
is less significant as the sequence level
In this paper, we investigated the teacher-student training for smallfootprint
acoustic models using HDNNs. We observed that the accuracy
of the student acoustic model could be improved under the supervision
of a high accuracy teacher model, even without additional
unsupervised data. In particular, the student model supervised by a
sMBR-based teacher model achieved lower WER compared to the
model trained independently using the sMBR-based sequence training
approach. Unsupervised speaker adaptation further improved the
recognition accuracy by around 5% relative for our model with less
then 0.8 million model parameters. However, we did not obtain improvements
by using the hybrid loss function by interpolating the CE
and teacher-student loss functions, and using higher temperature to
smooth the pseudo labels did not help either. In the future, we shall
evaluate this model on low resource conditions where the amount of
training data is much smaller.years have witnessed wide applications of speech technology
in embedded devices like mobile phones, thanks to deep learning
that has significantly advanced state-of-the-art in this area. For scenarios
that internet connections are unavailable or for privacy concerns,
it is desirable that speech recognisers can run locally in such
kind of resource constrained platforms. However, state-of-the-art
neural network models are either computationally expensive or consume
large amount of memory, and are therefore unsuitable for this
purpose. Recently, there have been a number of works on small
footprint acoustic models to address this problem such as using lowrank
matrices [1, 2], structured linear layers [3, 4, 5], and the use
of low rank displacement of structured matrices [4]. Instead of manipulating
the model parameters, another approach is based on the
teacher-student architecture [6, 7, 8], which is also known as model
compression [9] or knowledge distillation [10]. In this approach, the
teacher may be a 
the convergence curves of using CE and sMBR based teacher models,
where we see that the student model achieves much higher framefor the standalone HDNN model, i.e., we decoded the evaluation utterances
to obtain the hard labels first, and then used these labels to
adapt the model using the CE loss as Eq. (7). However, using the
teacher-student loss as Eq. (8), only one pass decoding is required
because the pseudo labels for adaptation are provided by the teacher
model, which does not need the word level transcription. This is a
particular advantage of the teacher-student training technique. However,
note that for resource constrained application scenarios, the student
model should be adapted offline, because otherwise the teacher
model needs to be accessed to generate the labels. This requires another
set of unlabelled speaker-dependent data for adaptation, but it
is usually not expensive to collect.
large-size network or an ensemble of several different
models, which is used to predict the soft targets for training the
student model that is much smaller. As pointed out in [10], the soft
targets provided by the teacher encode the generalisation power ofMake your own implementation of a Bloom filter. We leave the choice of a hashing function up to you.
Write a program that uses your implementation of Bloom filter to approximately de-duplicate the lines
in the file webLarge.txt. The output of an approximate de-duplication contains no duplicate lines, but
some lines from the input might not appear at all in the output. You should think carefully about the
number of hashing functions and the size of the Bloom filter you use. The probability that a line (and its
duplicates) from the input does not appear at all in the output should be less than 1%. You can assume
that your hashing functions produce every value equally likely. When choosing the size of your Bloom
filter and number of hashing functions, you should assume the worst case in which all lines are unique.
The number of lines should be a command-line parameter to your program.Task 4 / Task
In lectures, you saw how to sample uniformly a single line from a stream of lines efficiently on a single
machine. For large data, running reservoir sampling on a single machine would take too long. Implement
a MapReduce version of reservoir sampling which uniformly samples only a single line and uses
MapReduce to do so. Use your implementation to sample a single line from the file webLarge.txt.
Your output should contain only a single line. Do not implement the method that generates a random
number for each line then takes the largest one. (3 marks)
Task 5 / Task
Extend the basic version of reservoir sampling such that it can sample multiple lines uniformly without
replacement and run on a single machine (i.e. no MapReduce). This means that if we want to sample
k lines from a file with n lines in total, then each of the (nk
) possible outcomes has equal probability.
Implement a program that will sample 100 lines from the file webLarge.txt. Run your program locally
(not as a MapReduce job).pieces of information. Initially, you should understand the format of the dataset, next you will need to
parse each post, and finally you will need to implement your MapReduce workflows. Use MapReduce
for tasks 2 and 3.
The dataset contains a number of post records, one record per line. Each record consists of commaseparated
key-value pairs, which are then pointlessly wrapped in an XML element. That is, a record
looks like:
<row attribute1=value1, attribute2=value2, ..., attributeN=valueN />
Each record has its own identifier stored in a field named Id and a type, indicated by the value of a field
PostTypeId. If the value of PostTypeId is 1, than the post refers to a question, otherwise is the value of
PostTypeId is 2 the post refers to an answer.
An example of a question post is:
<row Id="2155" PostTypeId="1" AcceptedAnswerId="2928"
CreationDate="2008-08-05T12:13:40.640" Score="25" ViewCount="17551"
Body="The question content" OwnerUserId="371" LastEditorUserId="2134"
LastEditorDisplayName="stackoverflowGuy"
LastEditDate="2008-08-23T18:09:09.777"
LastActivityDate="2013-09-19T15:39:43.160" Title="How do I?"
Tags="&lt;asp.net&gt;" AnswerCount="6" CommentCount="0"
FavoriteCount="12" />
You will need to parse the record into a structure that will allow access to the value of each attribute
by name. In this example, Id="2155" represents the unique identifier given to the post; PostTypeId="1"
means that this post is a question; AcceptedAnswerId="2928" means that the accepted answer from the
user for this query is the answer with Id="2928"; and so on.
An example of a post that corresponds to an answer is:
<row Id="659891" PostTypeId="2" ParentId="659089"
CreationDate="2009-03-18T20:07:44.843" Score="1"
Body="Description of the problem"
OwnerUserId="45756" OwnerDisplayName="terminator"
LastActivityDate="2009-03-18T20:07:44.843" CommentCount="0" />We may use a bot to kill long-running jobs so that everyone can run on the cluster.
This assignment is divided into three parts and eight tasks. The first part deals with using MapReduce
for building an inverted index. The second part deals with parsing and analysing data from
StackOverflow. The last part deals with randomized and approximate algorithms. In all tasks, except
7, it is ok to use the output of a previous task as input to subsequent tasks.
You should use the teaching Hadoop Cluster1 and any programming language you want. If you are
logging in from outside Informatics, first
ssh s12345678@student.ssh.inf.ed.ac.uk
or use the Informatics VPN:
http://computing.help.inf.ed.ac.uk/openvpn
Once inside Informatics, connect to a random machine in the cluster:
ssh scutter$(seq -w 1 12 | shuf -n 1)
For each part there are different data sets (on HDFS). There are two versions of each input file that
should be used in your program, a small one and a larger one. The small version file is for developing
and testing your code; when you are happy that it works, you should use the large version.
Reference outputs for small data